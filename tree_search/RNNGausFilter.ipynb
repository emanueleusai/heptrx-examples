{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track filtering/fitting with LSTMs and Gaussian predictions\n",
    "\n",
    "This is a continuous space model using the ACTS data.\n",
    "Like the LSTMTrackFilter notebook, it predicts the location of the next hit given a hit sequence.\n",
    "However, unlike the previous model, we now produce a probability distribution in the form of a multivariate Gaussian.\n",
    "\n",
    "This lets our model quantify uncertainty and score hits in a more meaningful way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a GPU first\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'\n",
    "cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Data libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local imports\n",
    "from data import process_files\n",
    "import torchutils\n",
    "torchutils.set_cuda(cuda)\n",
    "from torchutils import np_to_torch, torch_zeros, torch_to_np\n",
    "from estimator import Estimator\n",
    "\n",
    "# Magic\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_hits(hits):\n",
    "    # Select all barrel hits\n",
    "    vids = [8, 13, 17]\n",
    "    barrel_hits = hits[np.logical_or.reduce([hits.volid == v for v in vids])]\n",
    "    # Re-enumerate the volume and layer numbers for convenience\n",
    "    volume = pd.Series(-1, index=barrel_hits.index, dtype=np.int8)\n",
    "    vid_groups = barrel_hits.groupby('volid')\n",
    "    for i, v in enumerate(vids):\n",
    "        volume[vid_groups.get_group(v).index] = i\n",
    "    # This assumes 4 layers per volume (except last volume)\n",
    "    layer = (barrel_hits.layid / 2 - 1 + volume * 4).astype(np.int8)\n",
    "    return (barrel_hits[['evtid', 'barcode', 'phi', 'z']]\n",
    "            .assign(volume=volume, layer=layer))\n",
    "\n",
    "def select_signal_hits(hits):\n",
    "    \"\"\"Select signal hits from tracks that hit all barrel layers\"\"\"\n",
    "    return (hits.groupby(['evtid', 'barcode'])\n",
    "            # Select tracks that hit every layer at least once\n",
    "            .filter(lambda x: len(x) >= 10 and x.layer.unique().size == 10)\n",
    "            # Average duplicate hits together\n",
    "            .groupby(['evtid', 'barcode', 'layer'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "data_dir = '/bigdata/shared/ACTS/prod_mu10_pt1000_2017_07_29'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files = 20\n",
    "\n",
    "all_files = os.listdir(data_dir)\n",
    "hits_files = sorted(f for f in all_files if f.startswith('clusters'))\n",
    "hits_files = [os.path.join(data_dir, f) for f in hits_files[:n_files]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_100.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_1.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_10.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_12.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_11.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_13.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_14.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_15.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_16.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_17.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_18.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_19.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_2.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_20.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_21.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_22.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_23.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_24.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_25.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_26.csv\n"
     ]
    }
   ],
   "source": [
    "n_workers = 5\n",
    "hits = process_files(hits_files, num_workers=n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits data shape: (10568614, 7)\n"
     ]
    }
   ],
   "source": [
    "print('Hits data shape:', hits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected barrel hits: (6655049, 6)\n",
      "signal track hits: (2736290, 6)\n",
      "CPU times: user 2min 35s, sys: 1.27 s, total: 2min 36s\n",
      "Wall time: 2min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Select hits\n",
    "selected_hits = select_hits(hits)\n",
    "print('selected barrel hits:', selected_hits.shape)\n",
    "signal_hits = select_signal_hits(selected_hits)\n",
    "print('signal track hits:', signal_hits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 38s, sys: 625 ms, total: 2min 39s\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Gather features into tensor of shape (events, layers, features)\n",
    "input_data = np.stack(signal_hits.groupby(['evtid', 'barcode'])\n",
    "                      .apply(lambda x: x[['phi', 'z', 'layer']].values)).astype(np.float32)\n",
    "\n",
    "# Scale coordinates to approx [-1, 1] for phi and z, [0, 1] for layer number\n",
    "coord_scale = np.array([np.pi, 1000., 10.])\n",
    "cov_scale = coord_scale[:2, None] * coord_scale[None, :2]\n",
    "input_data[:,:] /= coord_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "We define an LSTM model in PyTorch which will predict the next hit location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cholesky(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Cholesky decomposition with gradient. Taken from\n",
    "    https://github.com/t-vi/pytorch-tvmisc/blob/master/misc/gaussian_process_regression_basic.ipynb\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, a):\n",
    "        l = torch.potrf(a, False)\n",
    "        ctx.save_for_backward(l)\n",
    "        return l\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        l, = ctx.saved_variables\n",
    "        # Gradient is l^{-H} @ ((l^{H} @ grad) * (tril(ones)-1/2*eye)) @ l^{-1}\n",
    "        # Ideally, this should use some form of solve triangular instead of inverse...\n",
    "        linv =  l.inverse()\n",
    "        \n",
    "        inner = (torch.tril(torch.mm(l.t(), grad_output)) * \n",
    "                 torch.tril(1.0 - Variable(l.data.new(l.size(1)).fill_(0.5).diag())))\n",
    "        s = torch.mm(linv.t(), torch.mm(inner, linv))\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackFilterer(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module for particle track state estimation and hit prediction.\n",
    "    \n",
    "    This module is an RNN which takes a sequence of hits and produces a\n",
    "    Gaussian shaped prediction for the location of the next hit.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim=5):\n",
    "        super(TrackFilterer, self).__init__()\n",
    "        input_dim = 3\n",
    "        output_dim = 2\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        out_size = input_dim * (input_dim + 3) / 2\n",
    "        self.fc = nn.Linear(hidden_dim, out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Might want to accept also the radius of the target layer.\"\"\"\n",
    "        input_size = x.size()\n",
    "        \n",
    "        # Initialize the LSTM hidden state\n",
    "        h = (torch_zeros(input_size[0], self.lstm.hidden_size),\n",
    "             torch_zeros(input_size[0], self.lstm.hidden_size))\n",
    "        # Apply the LSTM module\n",
    "        x, h = self.lstm(x, h)\n",
    "        # Squash layer axis into batch axis\n",
    "        x = x.contiguous().view(-1, x.size(-1))\n",
    "        # Apply linear layer\n",
    "        output = self.fc(x)\n",
    "        \n",
    "        # Extract and transform the gaussian parameters\n",
    "        means = output[:, :2]\n",
    "        variances = output[:, 2:4].exp()\n",
    "        correlations = output[:, 4].tanh()\n",
    "        \n",
    "        # Construct the covariance matrix\n",
    "        covs = torch.bmm(variances[:, :, None], variances[:, None, :]).sqrt()\n",
    "        covs[:, 0, 1] = covs[:, 0, 1].clone() * correlations\n",
    "        covs[:, 1, 0] = covs[:, 1, 0].clone() * correlations\n",
    "        \n",
    "        # Expand the layer axis again, just for consistency/interpretability\n",
    "        means = means.contiguous().view(input_size[0], input_size[1], 2)\n",
    "        covs = covs.contiguous().view(input_size[0], input_size[1], 2, 2)\n",
    "        return means, covs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_data(data):\n",
    "    \"\"\"\n",
    "    Put a chunk of numpy data into format for the model.\n",
    "    Reshapes and slices out the input and target features,\n",
    "    and converts into PyTorch Variable format.    \n",
    "    \"\"\"\n",
    "    # All but last detector layer used as inputs\n",
    "    inputs = np_to_torch(data[:, :-1])\n",
    "    # Target includes all but first layer, and drops the layer feature\n",
    "    targets = np_to_torch(data[:, 1:, :-1])\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaus_llh_loss(outputs, targets):\n",
    "    \"\"\"Custom gaussian log-likelihood loss function\"\"\"\n",
    "    means, covs = outputs\n",
    "    # Flatten layer axis into batch axis to use batch matrix operations\n",
    "    means = means.contiguous().view(means.size(0)*means.size(1), means.size(2))\n",
    "    covs = covs.contiguous().view(covs.size(0)*covs.size(1), covs.size(2), covs.size(3))\n",
    "    targets = targets.contiguous().view(targets.size(0)*targets.size(1), targets.size(2))\n",
    "    # Calculate the inverses of the covariance matrices\n",
    "    inv_covs = torch.stack([cov.inverse() for cov in covs])\n",
    "    # Calculate the residual error\n",
    "    # TODO: need to fix for phi discontinuity!!\n",
    "    res = targets - means\n",
    "    # Calculate the residual error term\n",
    "    res_right = torch.bmm(inv_covs, res.unsqueeze(-1)).squeeze(-1)\n",
    "    res_term = torch.bmm(res[:,None,:], res_right[:,:,None]).squeeze()\n",
    "    # For the determinant term, we first have to compute the cholesky roots\n",
    "    diag_chols = torch.stack([Cholesky.apply(cov).diag() for cov in covs])\n",
    "    log_det = diag_chols.log().sum(1) * 2\n",
    "    gllh_loss = (res_term + log_det).sum()\n",
    "    return gllh_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config\n",
    "hidden_dim = 20\n",
    "\n",
    "# Train config\n",
    "n_epochs = 50\n",
    "batch_size = 64\n",
    "test_frac = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and test sets\n",
    "train_data, test_data = train_test_split(input_data, test_size=test_frac)\n",
    "# Final preparation of data for the model\n",
    "train_input, train_target = prepare_model_data(train_data)\n",
    "test_input, test_target = prepare_model_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrackFilterer (\n",
      "  (lstm): LSTM(3, 20, batch_first=True)\n",
      "  (fc): Linear (20 -> 9)\n",
      ")\n",
      "Parameters: 2189\n"
     ]
    }
   ],
   "source": [
    "# Construct the model and estimator\n",
    "estimator = Estimator(\n",
    "    TrackFilterer(hidden_dim=hidden_dim),\n",
    "    loss_func=gaus_llh_loss, cuda=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 246266\n",
      "Batches per epoch: 3848\n",
      "Validation samples: 27363\n",
      "Epoch 0\n",
      "  training loss -6.7e+03 time 754.702s\n",
      "  validate loss -3.66e+06\n",
      "Epoch 1\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "estimator.fit(train_input, train_target,\n",
    "              valid_input=test_input, valid_target=test_target,\n",
    "              batch_size=batch_size, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "epochs = np.arange(n_epochs)\n",
    "p1, = plt.plot(epochs, np.array(estimator.train_losses), c='b', label='Training set')\n",
    "plt.ylabel('Train loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.twinx()\n",
    "p2, = plt.plot(epochs, np.array(estimator.valid_losses), c='r', label='Test set')\n",
    "plt.ylabel('Test loss')\n",
    "plt.title('Test lost')\n",
    "plt.legend([p1, p2], ['Training set', 'Test set'])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance\n",
    "\n",
    "Understanding the loss is a bit difficult. I need to provide some additional metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = estimator.model(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick out the values to draw\n",
    "isample = 0\n",
    "mus = torch_to_np(test_output[0][isample]) * coord_scale[:2]\n",
    "covs = torch_to_np(test_output[1][isample]) * cov_scale\n",
    "targets = torch_to_np(test_target[isample]) * coord_scale[:2]\n",
    "\n",
    "# Draw figure with 9 subplots (3x3) for the predictions on each layer\n",
    "plt.figure(figsize=(9,8))\n",
    "\n",
    "for ilay in range(9):\n",
    "    plt.subplot(3, 3, ilay+1)\n",
    "    # Construct a gaussian from our prediction\n",
    "    pred = multivariate_normal(mus[ilay], covs[ilay])\n",
    "    \n",
    "    # Compute values on a grid\n",
    "    wsize = np.array([np.pi/16, 100])\n",
    "    wlow, whigh = mus[ilay] - wsize, mus[ilay] + wsize\n",
    "    phi_grid, z_grid = np.mgrid[wlow[0]:whigh[0]:.01, wlow[1]:whigh[1]:1]\n",
    "    pred_grid = pred.pdf(np.dstack([phi_grid, z_grid]))\n",
    "    \n",
    "    # Draw the predicted gaussian\n",
    "    c = plt.contour(phi_grid, z_grid, pred_grid)\n",
    "    # Draw the actual hit location\n",
    "    plt.scatter(targets[ilay, 0], targets[ilay, 1])\n",
    "    # Draw the mean prediction\n",
    "    plt.scatter(mus[ilay, 0], mus[ilay, 1], marker='+')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = np.arange(10)\n",
    "\n",
    "# Loop over a few samples to draw\n",
    "for isample in range(4):\n",
    "\n",
    "    inputs = torch_to_np(test_input[isample]) * coord_scale\n",
    "    targets = torch_to_np(test_target[isample]) * coord_scale[:2]\n",
    "    pred_mus = torch_to_np(test_output[0][isample]) * coord_scale[:2]\n",
    "    pred_covs = torch_to_np(test_output[1][isample]) * cov_scale\n",
    "\n",
    "    pred_phi = pred_mus[:, 0]\n",
    "    pred_z = pred_mus[:, 1]\n",
    "    pred_phi_sig = np.sqrt(pred_covs[:,0,0])\n",
    "    pred_z_sig = np.sqrt(pred_covs[:,1,1])\n",
    "\n",
    "    # Draw the means first\n",
    "    plt.figure(figsize=(9,3))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(layers[:-1], inputs[:, 0], 'b.-')\n",
    "    plt.plot(layers[1:], targets[:,0], 'b.-', label='Data')\n",
    "    plt.errorbar(layers[1:], pred_phi, yerr=pred_phi_sig, fmt='r.', label='Filter')\n",
    "    plt.xlabel('layer')\n",
    "    plt.ylabel('phi [rad]')\n",
    "    plt.legend(loc=0)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(layers[:-1], inputs[:, 1], 'b.-')\n",
    "    plt.plot(layers[1:], targets[:, 1], 'b.-', label='Data')\n",
    "    plt.errorbar(layers[1:], pred_z, yerr=pred_z_sig, fmt='r.', label='Filter')\n",
    "    plt.xlabel('layer')\n",
    "    plt.ylabel('z [mm]')\n",
    "    plt.legend(loc=0)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual error and pull distributions\n",
    "\n",
    "Draw the error-weighted mean difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_residuals(preds, targets):\n",
    "    \"\"\"\n",
    "    Calculates the residuals errors for phi and z coordinates.\n",
    "    Corrects for the delta-phi discontinuity.\n",
    "    \"\"\"\n",
    "    res = preds - targets\n",
    "    # fix the delta-phi calculation around the discontinuity\n",
    "    res[res[:,:,0] > np.pi, 0] -= 2*np.pi\n",
    "    res[res[:,:,0] < -np.pi, 0] += 2*np.pi\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the test set data, errors, pulls\n",
    "preds = torch_to_np(test_output[0]) * coord_scale[:2]\n",
    "sigmas = np.sqrt(np.diagonal(torch_to_np(test_output[1]), axis1=2, axis2=3)) * coord_scale[:2]\n",
    "targets = torch_to_np(test_target) * coord_scale[:2]\n",
    "residuals = calc_residuals(preds, targets)\n",
    "pulls = residuals / sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pull distributions\n",
    "phi_pulls = pulls[:, :, 0].flatten()\n",
    "z_pulls = pulls[:, :, 1].flatten()\n",
    "fit_x = np.linspace(-5, 5)\n",
    "\n",
    "phi_pull_mu, phi_pull_std = norm.fit(phi_pulls)\n",
    "phi_pull_fit = norm.pdf(fit_x, phi_pull_mu, phi_pull_std)\n",
    "\n",
    "z_pull_mu, z_pull_std = norm.fit(z_pulls)\n",
    "z_pull_fit = norm.pdf(fit_x, z_pull_mu, z_pull_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the residuals\n",
    "plt.figure(figsize=(9,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.hist(residuals[:, :, 0].flatten(), bins=50, log=True)\n",
    "plt.xlabel('$\\phi - \\phi_{true}$ [rad]')\n",
    "plt.title('$\\phi$ residual error')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(residuals[:, :, 1].flatten(), bins=50, log=True)\n",
    "plt.xlabel('$z - z_{true}$ [mm]')\n",
    "plt.title('Z residual error')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Draw the pull distributions\n",
    "plt.figure(figsize=(9,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.hist(phi_pulls, bins=50, range=[-5, 5], normed=True)\n",
    "fit_label = 'Gaussian fit\\n$\\mu$ = %.3f\\n$\\sigma$ = %.3f' % (phi_pull_mu, phi_pull_std)\n",
    "plt.plot(fit_x, phi_pull_fit, label=fit_label)\n",
    "plt.xlabel('$(\\phi - \\phi_{true}) / \\sigma_\\phi$')\n",
    "plt.title('$\\phi$ pull distribution')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(z_pulls, bins=50, range=[-5, 5], normed=True)\n",
    "fit_label = 'Gaussian fit\\n$\\mu$ = %.3f\\n$\\sigma$ = %.3f' % (z_pull_mu, z_pull_std)\n",
    "plt.plot(fit_x, z_pull_fit, label=fit_label)\n",
    "plt.xlabel('$(z - z_{true}) / \\sigma_z$')\n",
    "plt.title('Z pull distribution')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "This model seems to be able to learn trajectories better than the simpler one which only predicts a central value with an MSE loss, which is a rather interesting thing.\n",
    "\n",
    "It's still not perfect, certainly. There out some outliers in the $\\phi$ residual at $\\pm \\pi$ that I need to investigate. The shapes of the errors and pulls vary a bit as I modify things and rerun, which suggests instability or lack of convergence. Usually the $\\phi$ pull distribution is highly assymetric (maybe even multi-modal), but in this current run it is actually fairly well behaved. Usually the z residuals and pull distribution have been very well behaved but in this case we see the pull distribution shows some bias.\n",
    "\n",
    "The loss during training seems a little unstable, which could be related to the biases observed above. I may be able to improve this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
