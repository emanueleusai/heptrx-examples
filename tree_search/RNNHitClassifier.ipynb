{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track candidate tree search with LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select a GPU first\n",
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '6'\n",
    "cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# System imports\n",
    "import ast\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Data libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local imports\n",
    "from data import load_data_events\n",
    "\n",
    "# Magic\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_hits_data(df, copy_keys=['evtid', 'barcode', 'volid', 'layid']):\n",
    "    \"\"\"Split columns and calculate some derived variables\"\"\"\n",
    "    x = df.gpos.apply(lambda pos: pos[0])\n",
    "    y = df.gpos.apply(lambda pos: pos[1])\n",
    "    z = df.gpos.apply(lambda pos: pos[2])\n",
    "    r = np.sqrt(x**2 + y**2)\n",
    "    phi = np.arctan2(y, x)\n",
    "    return df[copy_keys].assign(z=z.astype(np.float32), r=r.astype(np.float32),\n",
    "                                phi=phi.astype(np.float32))\n",
    "\n",
    "def read_worker(hits_file):\n",
    "    hits_columns = ['hitid', 'barcode', 'volid', 'layid', 'lpos',\n",
    "                    'lerr', 'gpos', 'chans', 'dir', 'direrr']\n",
    "    return process_hits_data(load_data_events(hits_file, columns=hits_columns))\n",
    "\n",
    "def process_files(hits_files, num_workers):\n",
    "    \"\"\"Load and process a set of hits files with MP\"\"\"\n",
    "    pool = mp.Pool(processes=num_workers)\n",
    "    hits = pool.map(read_worker, hits_files)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    # Fix the evtid to be consecutive\n",
    "    for i in range(1, len(hits)):\n",
    "        hits[i].evtid += hits[i-1].evtid.iloc[-1] + 1\n",
    "    return pd.concat(hits, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_eta(r, z):\n",
    "    theta = np.arctan2(r, z)\n",
    "    return -1. * np.log(np.tan(theta / 2.))\n",
    "\n",
    "def calc_dphi(phi1, phi2):\n",
    "    dphi = np.abs(phi1 - phi2)\n",
    "    idx = dphi > np.pi\n",
    "    dphi[idx] = 2*np.pi - dphi[idx]\n",
    "    return dphi\n",
    "\n",
    "def calc_dR(eta1, eta2, phi1, phi2):\n",
    "    deta = np.abs(eta1 - eta2)\n",
    "    dphi = calc_dphi(phi1, phi2)\n",
    "    return np.sqrt(deta*deta + dphi*dphi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PyTorch memory allocations and conversions\n",
    "np_to_torch_cpu = lambda x: Variable(torch.from_numpy(x))\n",
    "np_to_torch_gpu = lambda x: Variable(torch.from_numpy(x)).cuda()\n",
    "torch_zeros_cpu = lambda *size: Variable(torch.FloatTensor(*size).zero_())\n",
    "torch_zeros_gpu = lambda *size: Variable(torch.cuda.FloatTensor(*size).zero_())\n",
    "\n",
    "np_to_torch = np_to_torch_gpu if cuda else np_to_torch_cpu\n",
    "torch_zeros = torch_zeros_gpu if cuda else torch_zeros_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_files = 10\n",
    "\n",
    "all_files = os.listdir(data_dir)\n",
    "hits_files = sorted(f for f in all_files if f.startswith('clusters'))\n",
    "hits_files = [os.path.join(data_dir, f) for f in hits_files[:n_files]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_100.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_12.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_11.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_10.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_1.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_13.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_14.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_15.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_16.csv\n",
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_17.csv\n"
     ]
    }
   ],
   "source": [
    "n_workers = 5\n",
    "hits = process_files(hits_files, num_workers=n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-step classification\n",
    "\n",
    "For the first iteration of a tree search algorithm, we will develop a model which reads a fixed number of steps (maybe 3) and classifies individual hits as being the next step of that track.\n",
    "\n",
    "Input:\n",
    "- A list of coordinates for the track hits\n",
    "- A list of next-hit candidate coordinates\n",
    "\n",
    "Target:\n",
    "- Binary labels for the candidate coordinates\n",
    "\n",
    "The track hits tensor will have shape (n_events, n_hits, n_features).\n",
    "The candidate hits tensor will have the same shape, just with a different number of hits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_hits(hits):\n",
    "    # Select the inner pixel layers\n",
    "    hits = hits[hits.volid == 8]\n",
    "    # Enumerate layers 0 to 3\n",
    "    layer = (hits.layid / 2 - 1).astype(np.int8)\n",
    "    # Select just the fields we need\n",
    "    hits = hits[['evtid', 'barcode', 'phi', 'r', 'z']].assign(layer=layer)\n",
    "    # Average all duplicate hits together\n",
    "    return hits.groupby(['evtid', 'barcode', 'layer'], as_index=False).mean()\n",
    "\n",
    "def select_signal_hits(hits):\n",
    "    \"\"\"Select tracks that hit every layer and have sufficient number of bkg hits\"\"\"\n",
    "    # Require at least 5 event hits on the candidate layer\n",
    "    hits = hits.groupby('evtid').filter(lambda x: (x.layer == 3).sum() >= 5)\n",
    "    # Require at least 1 signal hit on every layer\n",
    "    return (hits.groupby(['evtid', 'barcode'])\n",
    "            .filter(lambda x: len(x) >= 4 and x.layer.unique().size == 4))\n",
    "\n",
    "def extract_features(x, feature_names, scale_factors):\n",
    "    return x[feature_names].values / scale_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 38s, sys: 723 ms, total: 1min 39s\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sel_hits = select_hits(hits)\n",
    "sig_hits = select_signal_hits(sel_hits)\n",
    "\n",
    "evt_groups = sel_hits.groupby('evtid')\n",
    "sig_groups = sig_hits.groupby(['evtid', 'barcode'])\n",
    "sig_keys = sig_groups.groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_names = ['r', 'phi', 'z']\n",
    "feature_scale_factors = [100., np.pi, 200.]\n",
    "get_features = partial(extract_features, feature_names=feature_names,\n",
    "                       scale_factors=feature_scale_factors)\n",
    "\n",
    "n_samples = len(sig_keys)\n",
    "n_trk_hits = 3\n",
    "n_cand_hits = 5\n",
    "n_features = len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the data structures\n",
    "trk_input = np.zeros((n_samples, n_trk_hits, n_features), dtype=np.float32)\n",
    "hit_input = np.zeros((n_samples, n_cand_hits, n_features), dtype=np.float32)\n",
    "hit_labels = np.zeros((n_samples, n_cand_hits), dtype=np.float32)\n",
    "\n",
    "# No need to shuffle; true hit is always first\n",
    "hit_labels[:,0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23min 10s, sys: 4.31 s, total: 23min 14s\n",
      "Wall time: 23min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Loop over samples\n",
    "for i in xrange(n_samples):\n",
    "    eid, pid = sig_keys[i]\n",
    "    # Select the signal and background hits\n",
    "    trk_hits = sig_groups.get_group((eid, pid))\n",
    "    evt_hits = evt_groups.get_group(eid)\n",
    "    bkg_hits = evt_hits[(evt_hits.layer == n_trk_hits) & (evt_hits.barcode != pid)]\n",
    "    \n",
    "    # Select closest background hits in eta-phi\n",
    "    sig_cand_hit = trk_hits.iloc[-1]\n",
    "    sig_cand_eta = calc_eta(sig_cand_hit.r, sig_cand_hit.z)\n",
    "    bkg_hits_eta = calc_eta(bkg_hits.r, bkg_hits.z)\n",
    "    bkg_dr = calc_dR(sig_cand_eta, bkg_hits_eta, sig_cand_hit.phi, bkg_hits.phi)\n",
    "    bkg_cand_hits = bkg_hits.loc[bkg_dr.sort_values().head(n_cand_hits - 1).index]\n",
    "    \n",
    "    # Extract the features\n",
    "    trk_features = get_features(trk_hits)\n",
    "    bkg_features = get_features(bkg_cand_hits)\n",
    "\n",
    "    # Fill the model inputs\n",
    "    trk_input[i] = trk_features[:n_trk_hits]\n",
    "    hit_input[i] = np.concatenate([trk_features[None, n_trk_hits], bkg_features])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%%time\n",
    "\n",
    "# Loop over samples\n",
    "for i in xrange(n_samples):\n",
    "    eid, pid = sig_keys[i]\n",
    "    # Select the signal and background hits\n",
    "    trk_hits = sig_groups.get_group((eid, pid))\n",
    "    evt_hits = evt_groups.get_group(eid)\n",
    "    bkg_hits = evt_hits[(evt_hits.layer == n_trk_hits) & (evt_hits.barcode != pid)]\n",
    "    # Extract the features\n",
    "    trk_features = get_features(trk_hits)\n",
    "    bkg_features = get_features(bkg_hits.sample(n_cand_hits - 1))\n",
    "    # Fill the model inputs\n",
    "    trk_input[i] = trk_features[:n_trk_hits]\n",
    "    hit_input[i] = np.concatenate([trk_features[None, n_trk_hits], bkg_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_distributed(module, x):\n",
    "    \"\"\"Applies a module across both batch and 'time' dimensions\"\"\"\n",
    "    s = x.size()\n",
    "    y = module(x.view((-1,) + s[2:]))\n",
    "    return y.view(s[:2] + y.size()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrackHitScorer(nn.Module):\n",
    "    \"\"\"\n",
    "    A track-hit binary classifier model.\n",
    "    \n",
    "    This model embeds a sequence of hits using an LSTM into a state estimate.\n",
    "    It then classifies candidate hits conditional on that state.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, state_dim, hidden_dims):\n",
    "        \"\"\"Initialize the model\"\"\"\n",
    "        super(TrackHitScorer, self).__init__()\n",
    "        \n",
    "        # Use an LSTM for the encoder\n",
    "        self.encoder = nn.LSTM(input_dim, state_dim, batch_first=True)\n",
    "        \n",
    "        # Fully-connected classifier hidden layers\n",
    "        clf_layers = [nn.Linear(input_dim + state_dim, hidden_dims[0]), nn.ReLU()]\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            clf_layers += [nn.Linear(hidden_dims[i], hidden_dims[i+1]), nn.ReLU()]\n",
    "        # Classifier final layer\n",
    "        clf_layers += [nn.Linear(hidden_dims[-1], 1)]\n",
    "        self.classifier = nn.Sequential(*clf_layers)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        trk_inputs, hit_inputs = inputs\n",
    "        trk_input_size = trk_inputs.size()\n",
    "        \n",
    "        # Initialize the lstm hidden state\n",
    "        var_args = [trk_input_size[0], self.encoder.hidden_size]\n",
    "        #var_args = [self.encoder.num_layers, trk_input_size[0], self.encoder.hidden_size]\n",
    "        h0, c0 = torch_zeros(*var_args), torch_zeros(*var_args)\n",
    "        \n",
    "        # Encode the track hits into a state estimate\n",
    "        _, (h, c) = self.encoder(trk_inputs, (h0, c0))\n",
    "        \n",
    "        # Broadcast state from shape (batch, state) to (batch, hits, state).\n",
    "        expanded_size = (h.size(1), hit_inputs.size(1), h.size(2))\n",
    "        states = h.squeeze(0)[:, None, :].expand(*expanded_size)\n",
    "        # Attach state estimate onto every hit candidate for classification.\n",
    "        x = torch.cat([hit_inputs, states], dim=-1)\n",
    "        \n",
    "        # Apply classifier head to every candidate\n",
    "        return time_distributed(self.classifier, x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_prob(model, inputs):\n",
    "    return F.sigmoid(model(inputs))\n",
    "\n",
    "def training_step(model, inputs, targets, loss_func, optimizer):\n",
    "    model.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_func(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def accuracy(probs, target, threshold=0.5):\n",
    "    return ((probs.data.numpy() > threshold) == (target.data.numpy() > 0.5)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model config\n",
    "state_size = 16\n",
    "hidden_sizes = [16, 16, 16]\n",
    "\n",
    "# Training config\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "test_frac = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 187698\n",
      "Batches per epoch: 2933\n",
      "Test samples: 20856\n"
     ]
    }
   ],
   "source": [
    "n_train = int(n_samples * (1 - test_frac))\n",
    "n_batches = (n_train + batch_size - 1) // batch_size\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_trk_input = np_to_torch(trk_input[:n_train])\n",
    "train_hit_input = np_to_torch(hit_input[:n_train])\n",
    "train_hit_labels = np_to_torch(hit_labels[:n_train])\n",
    "test_trk_input = np_to_torch(trk_input[n_train:])\n",
    "test_hit_input = np_to_torch(hit_input[n_train:])\n",
    "test_hit_labels = np_to_torch(hit_labels[n_train:])\n",
    "\n",
    "print('Training samples:', n_train)\n",
    "print('Batches per epoch:', n_batches)\n",
    "print('Test samples:', n_samples - n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrackHitScorer (\n",
      "  (encoder): LSTM(3, 16, batch_first=True)\n",
      "  (classifier): Sequential (\n",
      "    (0): Linear (19 -> 16)\n",
      "    (1): ReLU ()\n",
      "    (2): Linear (16 -> 16)\n",
      "    (3): ReLU ()\n",
      "    (4): Linear (16 -> 16)\n",
      "    (5): ReLU ()\n",
      "    (6): Linear (16 -> 1)\n",
      "  )\n",
      ")\n",
      "Parameters: 2225\n"
     ]
    }
   ],
   "source": [
    "# Construct the model\n",
    "model = TrackHitScorer(input_dim=n_features, state_dim=state_size, hidden_dims=hidden_sizes)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(model)\n",
    "print('Parameters:', sum(param.numel() for param in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "  training loss 0.251 time 19.8075s\n",
      "  test loss 0.116\n",
      "Epoch 1\n",
      "  training loss 0.0807 time 19.6965s\n",
      "  test loss 0.059\n",
      "Epoch 2\n",
      "  training loss 0.0469 time 19.8309s\n",
      "  test loss 0.0335\n",
      "Epoch 3\n",
      "  training loss 0.0403 time 20.6198s\n",
      "  test loss 0.033\n",
      "Epoch 4\n",
      "  training loss 0.0383 time 20.0436s\n",
      "  test loss 0.0301\n",
      "Epoch 5\n",
      "  training loss 0.0375 time 19.3751s\n",
      "  test loss 0.0348\n",
      "Epoch 6\n",
      "  training loss 0.0331 time 19.8245s\n",
      "  test loss 0.022\n",
      "Epoch 7\n",
      "  training loss 0.0323 time 19.8075s\n",
      "  test loss 0.0228\n",
      "Epoch 8\n",
      "  training loss 0.0311 time 20.4844s\n",
      "  test loss 0.0202\n",
      "Epoch 9\n",
      "  training loss 0.0287 time 19.2873s\n",
      "  test loss 0.0211\n"
     ]
    }
   ],
   "source": [
    "batch_idxs = np.arange(0, n_train, batch_size)\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    print('Epoch', i)\n",
    "    start_time = timer()\n",
    "    sum_loss = 0\n",
    "\n",
    "    for j in batch_idxs:\n",
    "        batch_trk_input = train_trk_input[j:j+batch_size]\n",
    "        batch_hit_input = train_hit_input[j:j+batch_size]\n",
    "        batch_inputs = [batch_trk_input, batch_hit_input]\n",
    "        batch_target = train_hit_labels[j:j+batch_size]\n",
    "        sum_loss += training_step(model, batch_inputs, batch_target, loss_func, optimizer)\n",
    "\n",
    "    end_time = timer()\n",
    "    avg_loss = sum_loss.cpu().data[0] / n_batches\n",
    "    train_losses.append(avg_loss)\n",
    "    print('  training loss %.3g' % avg_loss, 'time %gs' % (end_time - start_time))\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    test_output = model([test_trk_input, test_hit_input])\n",
    "    test_loss = loss_func(test_output, test_hit_labels).cpu().data[0]\n",
    "    test_losses.append(test_loss)\n",
    "    print('  test loss %.3g' % test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99431338703490602"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate accuracy on the test set\n",
    "test_probs = F.sigmoid(test_output)\n",
    "accuracy(test_probs, test_hit_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "This works pretty well so far. What next?\n",
    "- make classifications along the entire length of a track\n",
    "- make it work with holes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop full track classification data\n",
    "\n",
    "Now I want to demonstrate that the model can learn to classify hits at any point along a track.\n",
    "\n",
    "Let's select candidate hits along the whole sequence of a track (maybe after a seed).\n",
    "\n",
    "The seed structure will be the same as before. The hit input will be of shape (batch, step, cand, features).\n",
    "\n",
    "I will use the same hit selection as was used in the track filter notebook: barrel hits, signal tracks that hit every layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def select_hits(hits):\n",
    "    # Select all barrel hits\n",
    "    vids = [8, 13, 17]\n",
    "    hits = hits[np.logical_or.reduce([hits.volid == v for v in vids])]\n",
    "    # Re-enumerate the volume and layer numbers for convenience\n",
    "    volume = pd.Series(-1, index=hits.index, dtype=np.int8)\n",
    "    vid_groups = hits.groupby('volid')\n",
    "    for i, v in enumerate(vids):\n",
    "        volume[vid_groups.get_group(v).index] = i\n",
    "    # This assumes 4 layers per volume (except last volume)\n",
    "    layer = (hits.layid / 2 - 1 + volume * 4).astype(np.int8)\n",
    "    hits = hits[['evtid', 'barcode', 'r', 'phi', 'z']].assign(layer=layer)\n",
    "    # Average all duplicate hits together\n",
    "    return hits.groupby(['evtid', 'barcode', 'layer'], as_index=False).mean()\n",
    "\n",
    "def select_signal_hits(hits):\n",
    "    \"\"\"Select signal hits from tracks that hit all barrel layers\"\"\"\n",
    "    sel_func = lambda x: len(x) >= 10 and x.layer.unique().size == 10\n",
    "    return hits.groupby(['evtid', 'barcode']).filter(sel_func)\n",
    "\n",
    "def extract_features(x, feature_names, scale_factors):\n",
    "    return x[feature_names].values / scale_factors\n",
    "\n",
    "def select_cand_hits(sig_hits, sel_hits, seed_length, feature_names, n_cand_hits):\n",
    "    # Drop seed layers before matching\n",
    "    sig_hits = sig_hits[sig_hits.layer >= seed_length]\n",
    "    sel_hits = sel_hits[sel_hits.layer >= seed_length]\n",
    "\n",
    "    # Calculate dR for all hits in every sample\n",
    "    paired_hits = sig_hits.merge(sel_hits, on=['evtid', 'layer'], suffixes=('_sig', ''))\n",
    "    eta = calc_eta(paired_hits.r, paired_hits.z)\n",
    "    sig_eta = calc_eta(paired_hits.r_sig, paired_hits.z_sig)\n",
    "    dR = calc_dR(eta, sig_eta, paired_hits.phi, paired_hits.phi_sig)\n",
    "    \n",
    "    # Select closest candidate hits\n",
    "    cand_group_cols = ['evtid', 'barcode_sig', 'layer']\n",
    "    return (paired_hits[cand_group_cols + feature_names].assign(dR=dR)\n",
    "            .groupby(cand_group_cols, as_index=False)\n",
    "            .apply(lambda x: x.nsmallest(n_cand_hits, 'dR')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 48s, sys: 1.04 s, total: 1min 49s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sel_hits = select_hits(hits)\n",
    "sig_hits = select_signal_hits(sel_hits)\n",
    "\n",
    "sig_groups = sig_hits.groupby(['evtid', 'barcode'])\n",
    "sig_keys = sig_groups.groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_names = ['r', 'phi', 'z']\n",
    "feature_scale_factors = [1000., np.pi, 1000.]\n",
    "get_features = partial(extract_features, feature_names=feature_names,\n",
    "                       scale_factors=feature_scale_factors)\n",
    "\n",
    "n_samples = len(sig_keys)\n",
    "track_length = 10\n",
    "seed_length = 3\n",
    "cand_length = track_length - seed_length\n",
    "n_cand_hits = 5\n",
    "n_features = len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the data structures\n",
    "trk_input = np.zeros((n_samples, track_length, n_features), dtype=np.float32)\n",
    "hit_input = np.zeros((n_samples, cand_length, n_cand_hits, n_features), dtype=np.float32)\n",
    "hit_labels = np.zeros((n_samples, cand_length, n_cand_hits), dtype=np.float32)\n",
    "\n",
    "# True hit is always first due to our sorting; no need to shuffle.\n",
    "hit_labels[:,0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min 43s, sys: 17.9 s, total: 19min 1s\n",
      "Wall time: 19min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cand_hits = select_cand_hits(sig_hits, sel_hits, seed_length, feature_names, n_cand_hits)\n",
    "cand_groups = cand_hits.groupby(['evtid', 'barcode_sig'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 6s, sys: 8.46 s, total: 17min 14s\n",
      "Wall time: 17min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Loop over samples\n",
    "for i in xrange(n_samples):\n",
    "    # Extract the hits\n",
    "    key = tuple(sig_keys[i])\n",
    "    sig_evt_hits = sig_groups.get_group(key)\n",
    "    cand_evt_hits = cand_groups.get_group(key)\n",
    "    \n",
    "    # Fill track features\n",
    "    trk_input[i] = get_features(sig_evt_hits)\n",
    "    \n",
    "    # Loop over candidate hit layers\n",
    "    for layer, lay_hits in cand_evt_hits.groupby('layer'):\n",
    "        # Translate layer number to layer index in tensor\n",
    "        l = layer - seed_length\n",
    "        # Fill the hit input features\n",
    "        hit_input[i, l, :lay_hits.shape[0]] = get_features(lay_hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_distributed(module, x):\n",
    "    \"\"\"Applies a module across both batch and 'time' dimensions\"\"\"\n",
    "    s = x.size()\n",
    "    y = module(x.view((-1,) + s[2:]))\n",
    "    return y.view(s[:2] + y.size()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrackHitScorer2(nn.Module):\n",
    "    \"\"\"\n",
    "    A track-hit binary classifier model.\n",
    "    \n",
    "    This model embeds a sequence of hits using an LSTM into a state estimate.\n",
    "    It then classifies candidate hits conditional on that state.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, state_dim, hidden_dims, seed_length):\n",
    "        \"\"\"Initialize the model\"\"\"\n",
    "        super(TrackHitScorer2, self).__init__()\n",
    "        \n",
    "        self.seed_length = seed_length\n",
    "        \n",
    "        # Use an LSTM for the encoder\n",
    "        self.encoder = nn.LSTM(input_dim, state_dim, batch_first=True)\n",
    "        \n",
    "        # Fully-connected classifier hidden layers\n",
    "        clf_layers = [nn.Linear(input_dim + state_dim, hidden_dims[0]), nn.ReLU()]\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            clf_layers += [nn.Linear(hidden_dims[i], hidden_dims[i+1]), nn.ReLU()]\n",
    "        # Classifier final layer\n",
    "        clf_layers += [nn.Linear(hidden_dims[-1], 1)]\n",
    "        self.classifier = nn.Sequential(*clf_layers)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        trk_inputs, hit_inputs = inputs\n",
    "        trk_input_size = trk_inputs.size()\n",
    "        \n",
    "        # Initialize the lstm hidden state\n",
    "        var_args = [trk_input_size[0], self.encoder.hidden_size]\n",
    "        h0, c0 = torch_zeros(*var_args), torch_zeros(*var_args)\n",
    "        \n",
    "        # Encode the track hits into a state estimate\n",
    "        states, _ = self.encoder(trk_inputs, (h0, c0))\n",
    "        # Drop the seed layer outputs\n",
    "        states = states[:, seed_length:]\n",
    "        \n",
    "        # Broadcast state from shape (batch, layer, state) to (batch, layer, hits, state).\n",
    "        expanded_size = (states.size(0), states.size(1), hit_inputs.size(2), states.size(2))\n",
    "        hit_states = states[:, :, None, :].expand(*expanded_size)\n",
    "        # Attach state estimates onto the hit candidates for classification.\n",
    "        clf_inputs = torch.cat([hit_inputs, hit_states], dim=-1)\n",
    "        \n",
    "        # Apply classifier head to every candidate\n",
    "        return time_distributed(self.classifier, clf_inputs).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_prob(model, inputs):\n",
    "    return F.sigmoid(model(inputs))\n",
    "\n",
    "def training_step(model, inputs, targets, loss_func, optimizer):\n",
    "    model.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_func(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def accuracy(probs, target, threshold=0.5):\n",
    "    return ((probs.data.numpy() > threshold) == (target.data.numpy() > 0.5)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model config\n",
    "state_size = 16\n",
    "hidden_sizes = [16, 16, 16]\n",
    "\n",
    "# Training config\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "test_frac = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 122388\n",
      "Batches per epoch: 1913\n",
      "Test samples: 13599\n"
     ]
    }
   ],
   "source": [
    "n_train = int(n_samples * (1 - test_frac))\n",
    "n_batches = (n_train + batch_size - 1) // batch_size\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_trk_input = np_to_torch(trk_input[:n_train])\n",
    "train_hit_input = np_to_torch(hit_input[:n_train])\n",
    "train_hit_labels = np_to_torch(hit_labels[:n_train])\n",
    "test_trk_input = np_to_torch(trk_input[n_train:])\n",
    "test_hit_input = np_to_torch(hit_input[n_train:])\n",
    "test_hit_labels = np_to_torch(hit_labels[n_train:])\n",
    "\n",
    "print('Training samples:', n_train)\n",
    "print('Batches per epoch:', n_batches)\n",
    "print('Test samples:', n_samples - n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrackHitScorer2 (\n",
      "  (encoder): LSTM(3, 16, batch_first=True)\n",
      "  (classifier): Sequential (\n",
      "    (0): Linear (19 -> 16)\n",
      "    (1): ReLU ()\n",
      "    (2): Linear (16 -> 16)\n",
      "    (3): ReLU ()\n",
      "    (4): Linear (16 -> 16)\n",
      "    (5): ReLU ()\n",
      "    (6): Linear (16 -> 1)\n",
      "  )\n",
      ")\n",
      "Parameters: 2225\n"
     ]
    }
   ],
   "source": [
    "# Construct the model\n",
    "model = TrackHitScorer2(input_dim=n_features, state_dim=state_size,\n",
    "                        hidden_dims=hidden_sizes, seed_length=seed_length)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(model)\n",
    "print('Parameters:', sum(param.numel() for param in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "  training loss 0.0408 time 28.4336s\n",
      "  test loss 1.48e-05\n",
      "Epoch 1\n",
      "  training loss 6.39e-06 time 29.2816s\n",
      "  test loss 2.45e-06\n",
      "Epoch 2\n",
      "  training loss 1.34e-06 time 28.6127s\n",
      "  test loss 6.57e-07\n",
      "Epoch 3\n",
      "  training loss 3.84e-07 time 37.0491s\n",
      "  test loss 2.02e-07\n",
      "Epoch 4\n",
      "  training loss 1.22e-07 time 30.6693s\n",
      "  test loss 6.53e-08\n",
      "Epoch 5\n",
      "  training loss 4.13e-08 time 30.5814s\n",
      "  test loss 1.72e-08\n",
      "Epoch 6\n",
      "  training loss 9.13e-09 time 30.3763s\n",
      "  test loss 3.61e-11\n",
      "Epoch 7\n",
      "  training loss 2.75e-11 time 29.8686s\n",
      "  test loss 2.3e-11\n",
      "Epoch 8\n",
      "  training loss 2.35e-11 time 29.6829s\n",
      "  test loss 2.3e-11\n",
      "Epoch 9\n",
      "  training loss 2.34e-11 time 30.2621s\n",
      "  test loss 2.3e-11\n"
     ]
    }
   ],
   "source": [
    "batch_idxs = np.arange(0, n_train, batch_size)\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    print('Epoch', i)\n",
    "    start_time = timer()\n",
    "    sum_loss = 0\n",
    "\n",
    "    for j in batch_idxs:\n",
    "        batch_trk_input = train_trk_input[j:j+batch_size]\n",
    "        batch_hit_input = train_hit_input[j:j+batch_size]\n",
    "        batch_inputs = [batch_trk_input, batch_hit_input]\n",
    "        batch_target = train_hit_labels[j:j+batch_size]\n",
    "        sum_loss += training_step(model, batch_inputs, batch_target, loss_func, optimizer)\n",
    "\n",
    "    end_time = timer()\n",
    "    avg_loss = sum_loss.cpu().data[0] / n_batches\n",
    "    train_losses.append(avg_loss)\n",
    "    print('  training loss %.3g' % avg_loss, 'time %gs' % (end_time - start_time))\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    test_output = model([test_trk_input, test_hit_input])\n",
    "    test_loss = loss_func(test_output, test_hit_labels).cpu().data[0]\n",
    "    test_losses.append(test_loss)\n",
    "    print('  test loss %.3g' % test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate accuracy on the entire training set\n",
    "train_output = model([train_trk_input, train_hit_input])\n",
    "train_probs = F.sigmoid(train_output)\n",
    "accuracy(train_probs, train_hit_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate accuracy on the test set\n",
    "test_probs = F.sigmoid(test_output)\n",
    "accuracy(test_probs, test_hit_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Wow, this works almost suspiciously well. Assuming there are no bugs, what is the next step?\n",
    "- figure out how to deal with holes?\n",
    "- move off-barrel?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [pytorch]",
   "language": "python",
   "name": "Python [pytorch]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
