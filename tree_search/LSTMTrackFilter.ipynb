{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track filtering/fitting with LSTMs\n",
    "\n",
    "This is a continuous space model using the ACTS data.\n",
    "\n",
    "This should eventually move into a more appropriate folder. It's just here now for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a GPU first\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# System imports\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Data libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local imports\n",
    "from data import process_files\n",
    "import torchutils\n",
    "torchutils.set_cuda(cuda)\n",
    "from torchutils import np_to_torch, torch_zeros, torch_to_np\n",
    "from estimator import Estimator\n",
    "\n",
    "# Magic\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_hits(hits):\n",
    "    # Select all barrel hits\n",
    "    vids = [8, 13, 17]\n",
    "    barrel_hits = hits[np.logical_or.reduce([hits.volid == v for v in vids])]\n",
    "    # Re-enumerate the volume and layer numbers for convenience\n",
    "    volume = pd.Series(-1, index=barrel_hits.index, dtype=np.int8)\n",
    "    vid_groups = barrel_hits.groupby('volid')\n",
    "    for i, v in enumerate(vids):\n",
    "        volume[vid_groups.get_group(v).index] = i\n",
    "    # This assumes 4 layers per volume (except last volume)\n",
    "    layer = (barrel_hits.layid / 2 - 1 + volume * 4).astype(np.int8)\n",
    "    return (barrel_hits[['evtid', 'barcode', 'phi', 'z']]\n",
    "            .assign(volume=volume, layer=layer))\n",
    "\n",
    "def select_signal_hits(hits):\n",
    "    \"\"\"Select signal hits from tracks that hit all barrel layers\"\"\"\n",
    "    return (hits.groupby(['evtid', 'barcode'])\n",
    "            # Select tracks that hit every layer at least once\n",
    "            .filter(lambda x: len(x) >= 10 and x.layer.unique().size == 10)\n",
    "            # Average duplicate hits together\n",
    "            .groupby(['evtid', 'barcode', 'layer'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29'\n",
    "#data_dir = '/bigdata/shared/ACTS/prod_mu10_pt1000_2017_07_29'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files = 1\n",
    "\n",
    "all_files = os.listdir(data_dir)\n",
    "hits_files = sorted(f for f in all_files if f.startswith('clusters'))\n",
    "hits_files = [os.path.join(data_dir, f) for f in hits_files[:n_files]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /global/cscratch1/sd/sfarrell/ACTS/prod_mu10_pt1000_2017_07_29/clusters_1.csv\n",
      "CPU times: user 39.5 ms, sys: 48.1 ms, total: 87.6 ms\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n_workers = 1\n",
    "hits = process_files(hits_files, num_workers=n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits data shape: (520540, 7)\n"
     ]
    }
   ],
   "source": [
    "print('Hits data shape:', hits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected barrel hits: (327771, 6)\n",
      "signal track hits: (134990, 6)\n",
      "CPU times: user 9.01 s, sys: 34.8 ms, total: 9.05 s\n",
      "Wall time: 9.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Select hits\n",
    "selected_hits = select_hits(hits)\n",
    "print('selected barrel hits:', selected_hits.shape)\n",
    "signal_hits = select_signal_hits(selected_hits)\n",
    "print('signal track hits:', signal_hits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 10s, sys: 1.51 s, total: 6min 12s\n",
      "Wall time: 6min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Gather features into tensor of shape (events, layers, features)\n",
    "input_data = np.stack(signal_hits.groupby(['evtid', 'barcode'])\n",
    "                      .apply(lambda x: x[['phi', 'z', 'layer']].values)).astype(np.float32)\n",
    "\n",
    "# Scale coordinates to approx [-1, 1] for phi and z, [0, 1] for layer number\n",
    "coord_scale = np.array([np.pi, 1000., 10.])\n",
    "input_data[:,:] /= coord_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "We define an LSTM model in PyTorch which will predict the next hit location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackFilterer(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=3, hidden_dim=5, output_dim=2, n_lstm_layers=1):\n",
    "        super(TrackFilterer, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_lstm_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_size = x.size()\n",
    "        # Initialize the lstm hidden state\n",
    "        h = (torch_zeros(self.lstm.num_layers, input_size[0], self.lstm.hidden_size),\n",
    "             torch_zeros(self.lstm.num_layers, input_size[0], self.lstm.hidden_size))\n",
    "        x, h = self.lstm(x, h)\n",
    "        # Flatten layer axis into batch axis so FC applies independently across layers.\n",
    "        x = (self.fc(x.contiguous().view(-1, x.size(-1)))\n",
    "             .view(input_size[0], input_size[1], -1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config\n",
    "hidden_dim = 20\n",
    "n_lstm_layers = 1\n",
    "\n",
    "# Train config\n",
    "n_epochs = 200\n",
    "batch_size = 32\n",
    "test_frac = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 615537\n",
      "Batches per epoch: 19236\n",
      "Test samples: 68394\n"
     ]
    }
   ],
   "source": [
    "# Split data into training, validation, and test sets\n",
    "train_data, test_data = train_test_split(input_data, test_size=test_frac)\n",
    "\n",
    "# Inputs are the hits from [0, N-1)\n",
    "# Targets are the hits from [1, N) without the layer feature.\n",
    "train_input = np_to_torch(train_data[:,:-1])\n",
    "train_target = np_to_torch(train_data[:,1:,:2])\n",
    "test_input = np_to_torch(test_data[:,:-1])\n",
    "test_target = np_to_torch(test_data[:,1:,:2])\n",
    "\n",
    "n_samples = train_input.size(0)\n",
    "n_batches = (n_samples + batch_size - 1) // batch_size\n",
    "print('Training samples:', n_samples)\n",
    "print('Batches per epoch:', n_batches)\n",
    "print('Test samples:', test_input.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrackFilterer (\n",
      "  (lstm): LSTM(3, 20, batch_first=True)\n",
      "  (fc): Linear (20 -> 2)\n",
      ")\n",
      "Parameters: 2042\n"
     ]
    }
   ],
   "source": [
    "# Construct the model and estimator\n",
    "estimator = Estimator(\n",
    "    TrackFilterer(hidden_dim=hidden_dim),\n",
    "    loss_func=nn.MSELoss(), cuda=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 615537\n",
      "Batches per epoch: 19236\n",
      "Validation samples: 68394\n",
      "Epoch 0\n",
      "  training loss 0.00662 time 157.933s\n",
      "  validate loss 0.00627\n",
      "Epoch 2\n",
      "  training loss 0.00601 time 163.919s\n",
      "  validate loss 0.00547\n",
      "Epoch 3\n",
      "  training loss 0.00544 time 163.744s\n",
      "  validate loss 0.00492\n",
      "Epoch 4\n",
      "  training loss 0.00502 time 159.187s\n",
      "  validate loss 0.00455\n",
      "Epoch 5\n",
      "  training loss 0.00465 time 158.514s\n",
      "  validate loss 0.00436\n",
      "Epoch 6\n",
      "  training loss 0.00433 time 165.388s\n",
      "  validate loss 0.0043\n",
      "Epoch 7\n",
      "  training loss 0.00407 time 162.893s\n",
      "  validate loss 0.00415\n",
      "Epoch 8\n",
      "  training loss 0.00387 time 166.608s\n",
      "  validate loss 0.00399\n",
      "Epoch 9\n",
      "  training loss 0.00367 time 165.504s\n",
      "  validate loss 0.00351\n",
      "Epoch 11\n",
      "  training loss 0.00347 time 166.627s\n",
      "  validate loss 0.00337\n",
      "Epoch 12\n",
      "  training loss 0.00328 time 166.973s\n",
      "  validate loss 0.00323\n",
      "Epoch 13\n",
      "  training loss 0.00317 time 166.277s\n",
      "  validate loss 0.00312\n",
      "Epoch 14\n",
      "  training loss 0.00308 time 166.048s\n",
      "  validate loss 0.00302\n",
      "Epoch 15\n",
      "  training loss 0.00299 time 159.786s\n",
      "  validate loss 0.00292\n",
      "Epoch 16\n",
      "  training loss 0.00291 time 164.037s\n",
      "  validate loss 0.00284\n",
      "Epoch 17\n",
      "  training loss 0.00284 time 160.837s\n",
      "  validate loss 0.00276\n",
      "Epoch 18\n",
      "  training loss 0.00277 time 159.312s\n",
      "  validate loss 0.00272\n",
      "Epoch 19\n",
      "  training loss 0.00271 time 160.268s\n",
      "  validate loss 0.00268\n",
      "Epoch 20\n",
      "  training loss 0.00266 time 162.054s\n",
      "  validate loss 0.00266\n",
      "Epoch 21\n",
      "  training loss 0.00261 time 167.132s\n",
      "  validate loss 0.00262\n",
      "Epoch 22\n",
      "  training loss 0.00256 time 164.275s\n",
      "  validate loss 0.00258\n",
      "Epoch 23\n",
      "  training loss 0.00252 time 165.368s\n",
      "  validate loss 0.00251\n",
      "Epoch 24\n",
      "  training loss 0.00248 time 167.439s\n",
      "  validate loss 0.00243\n",
      "Epoch 25\n",
      "  training loss 0.00246 time 165.583s\n",
      "  validate loss 0.00231\n",
      "Epoch 26\n",
      "  training loss 0.00244 time 166.443s\n",
      "  validate loss 0.00226\n",
      "Epoch 27\n",
      "  training loss 0.0024 time 168.21s\n",
      "  validate loss 0.00219\n",
      "Epoch 28\n",
      "  training loss 0.0024 time 167.322s\n",
      "  validate loss 0.00215\n",
      "Epoch 29\n",
      "  training loss 0.00238 time 166.269s\n",
      "  validate loss 0.0021\n",
      "Epoch 30\n",
      "  training loss 0.00248 time 167.482s\n",
      "  validate loss 0.00208\n",
      "Epoch 31\n",
      "  training loss 0.00234 time 168.298s\n",
      "  validate loss 0.00203\n",
      "Epoch 32\n",
      "  training loss 0.00236 time 165.677s\n",
      "  validate loss 0.002\n",
      "Epoch 33\n",
      "  training loss 0.0023 time 166.159s\n",
      "  validate loss 0.00198\n",
      "Epoch 34\n",
      "  training loss 0.00229 time 166.175s\n",
      "  validate loss 0.00196\n",
      "Epoch 35\n",
      "  training loss 0.0022 time 173.779s\n",
      "  validate loss 0.00193\n",
      "Epoch 36\n",
      "  training loss 0.00213 time 167.229s\n",
      "  validate loss 0.0019\n",
      "Epoch 37\n",
      "  training loss 0.00209 time 171.172s\n",
      "  validate loss 0.00188\n",
      "Epoch 38\n",
      "  training loss 0.00207 time 170.047s\n",
      "  validate loss 0.00185\n",
      "Epoch 39\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train the model\n",
    "estimator.fit(train_input, train_target,\n",
    "              valid_input=test_input, valid_target=test_target,\n",
    "              batch_size=batch_size, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "epochs = np.arange(n_epochs)\n",
    "plt.semilogy(epochs, np.array(estimator.train_losses), label='Training set')\n",
    "plt.semilogy(epochs, np.array(estimator.valid_losses), label='Validation set')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training loss')\n",
    "plt.legend(loc=0)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions and errors for the full training set\n",
    "train_output = estimator.model(train_input)\n",
    "train_error = train_output - train_target\n",
    "train_resid = train_error.cpu().data.numpy() * coord_scale[:2]\n",
    "\n",
    "# Get the predictions and errors for the full test set\n",
    "test_output = estimator.model(test_input)\n",
    "test_error = test_output - test_target\n",
    "test_resid = test_error.cpu().data.numpy() * coord_scale[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,4))\n",
    "plt.subplot(121)\n",
    "hist_args = dict(bins=50, range=(-0.2, 0.2), normed=True, log=False, histtype='step')\n",
    "plt.hist(train_resid[:,:,0].flatten(), label='Training set', **hist_args)\n",
    "plt.hist(test_resid[:,:,0].flatten(), label='Test set', **hist_args)\n",
    "plt.xlabel('Error in $\\phi$ [rad]')\n",
    "plt.ylabel('Normalized sample counts')\n",
    "plt.legend(loc=0)\n",
    "\n",
    "plt.subplot(122)\n",
    "hist_args = dict(bins=50, range=(-100, 100), normed=True, log=False, histtype='step')\n",
    "plt.hist(train_resid[:,:,1].flatten(), label='Training set', **hist_args)\n",
    "plt.hist(test_resid[:,:,1].flatten(), label='Test set', **hist_args)\n",
    "plt.xlabel('Error in z [mm]')\n",
    "plt.ylabel('Normalized sample counts')\n",
    "plt.legend(loc=0)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- The $\\phi$ residual has a funny asymmetric shape. Not sure what's going on there.\n",
    "- Training and test set agree perfectly.\n",
    "- Non-gaussian tails in the Z residual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    layers = np.arange(10)\n",
    "    inputs = test_input[i].cpu().data.numpy() * coord_scale\n",
    "    outputs = test_output[i].cpu().data.numpy() * coord_scale[:2]\n",
    "    targets = test_target[i].cpu().data.numpy() * coord_scale[:2]\n",
    "\n",
    "    plt.figure(figsize=(9,3))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.plot(layers[:-1], inputs[:,0], 'b.-')\n",
    "    plt.plot(layers[1:], targets[:,0], 'b.-', label='Data')\n",
    "    plt.plot(layers[1:], outputs[:,0], 'r.-', label='Filter')\n",
    "    plt.xlabel('Detector layer')\n",
    "    plt.ylabel('$\\phi$ [rad]')\n",
    "    plt.legend(loc=0)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(layers[:-1], inputs[:,1], 'b.-')\n",
    "    plt.plot(layers[1:], targets[:,1], 'b.-', label='Data')\n",
    "    plt.plot(layers[1:], outputs[:,1], 'r.-', label='Filter')\n",
    "    plt.xlabel('Detector layer')\n",
    "    plt.ylabel('z [mm]')\n",
    "    plt.legend(loc=0)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- Wrap-around at $\\phi = \\pi$ can screw up the filter (not currently shown).\n",
    "- Filter is often smoother than data in the coarse outer layers.\n",
    "- Few examples seem a bit unstable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
