{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM track finder for 2D toy data in PyTorch\n",
    "\n",
    "Here, I'm going to get familiar with PyTorch by reproducing the models in LSTM_Toy2D.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data import generate_straight_tracks, generate_uniform_noise, generate_track_bkg\n",
    "from drawing import draw_2d_event, draw_2d_input_and_pred\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Detector parameters\n",
    "det_width = 50\n",
    "det_depth = 50\n",
    "det_shape = (det_depth, det_width)\n",
    "seed_size = 5\n",
    "\n",
    "# Data config\n",
    "n_events = 102400\n",
    "n_bkg_tracks = 5\n",
    "noise_prob = 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def batch_generator(num_events, det_shape, num_bkg_tracks, noise_prob,\n",
    "                    seed_size, binary=True):\n",
    "    while True:\n",
    "        # Generate signal tracks in the masked region\n",
    "        sig_tracks = generate_straight_tracks(num_events, det_shape)\n",
    "        # Generate track background\n",
    "        bkg_tracks = generate_track_bkg(num_events, det_shape,\n",
    "                                        tracks_per_event=num_bkg_tracks,\n",
    "                                        skip_layers=seed_size)\n",
    "        # Generate noise background\n",
    "        noise = generate_uniform_noise(num_events, det_shape,\n",
    "                                       prob=noise_prob)\n",
    "        # Combine into full events\n",
    "        events = sig_tracks + bkg_tracks + noise\n",
    "        if binary:\n",
    "            events[events > 1] = 1\n",
    "\n",
    "        # Mask the data\n",
    "        yield events, sig_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "sig_tracks = generate_straight_tracks(n_events, det_shape).astype(np.float32)\n",
    "bkg_tracks = generate_track_bkg(n_events, det_shape,\n",
    "                                tracks_per_event=n_bkg_tracks,\n",
    "                                skip_layers=seed_size).astype(np.float32)\n",
    "noise = generate_uniform_noise(n_events, det_shape, prob=noise_prob).astype(np.float32)\n",
    "events = sig_tracks + bkg_tracks + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMTrackFinder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(LSTMTrackFinder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, 1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        # This might be slow, could be improved.\n",
    "        # Trying this without the softmax, for the loss calculation.\n",
    "        x = torch.stack([self.fc(x[:,i]) for i in range(x.size(1))], dim=1)\n",
    "        #x = torch.stack([F.softmax(self.fc(x[:,i])) for i in range(x.size(1))], dim=1)\n",
    "        return x\n",
    "\n",
    "def logits_to_probs(logits):\n",
    "    size = logits.size()\n",
    "    return F.softmax(logits.view(-1, size[-1])).view(size)\n",
    "\n",
    "def cost_function(logits, labels):\n",
    "    # Flatten the batch and detector layer dimensions\n",
    "    flat_logits = logits.view(-1, logits.size(-1))\n",
    "    flat_labels = labels.view(-1)\n",
    "    return F.cross_entropy(flat_logits, flat_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_input = Variable(torch.from_numpy(events))\n",
    "train_labels = Variable(torch.from_numpy(sig_tracks.argmax(axis=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model config\n",
    "hidden_dim = 100\n",
    "\n",
    "# Train config\n",
    "n_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "n_samples = len(train_input)\n",
    "n_batches = (n_samples + batch_size - 1) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMTrackFinder (\n",
      "  (lstm): LSTM(50, 100, batch_first=True)\n",
      "  (fc): Linear (100 -> 50)\n",
      ")\n",
      "Parameters: 65850\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = LSTMTrackFinder(det_width, hidden_dim)\n",
    "print(model)\n",
    "print('Parameters:',sum(param.numel() for param in model.parameters()))\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "#optimizer = torch.optim.RMSprop(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 50]) 20000\n",
      "torch.Size([400, 100]) 40000\n",
      "torch.Size([400]) 400\n",
      "torch.Size([400]) 400\n",
      "torch.Size([50, 100]) 5000\n",
      "torch.Size([50]) 50\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.size(), param.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "  average loss 2.16838989258 time 163.721s\n",
      "Epoch 1\n",
      "  average loss 1.53918838501 time 164.938s\n",
      "Epoch 2\n",
      "  average loss 1.35406570435 time 161.92s\n",
      "Epoch 3\n",
      "  average loss 1.2037386322 time 165.939s\n",
      "Epoch 4\n",
      "  average loss 1.09688018799 time 158.775s\n",
      "Epoch 5\n",
      "  average loss 0.954774932861 time 158.378s\n",
      "Epoch 6\n",
      "  average loss 0.876244430542 time 163.463s\n",
      "Epoch 7\n",
      "  average loss 0.776243286133 time 162.096s\n",
      "Epoch 8\n",
      "  average loss 0.747665252686 time 158.022s\n",
      "Epoch 9\n",
      "  average loss 0.682240219116 time 158.33s\n"
     ]
    }
   ],
   "source": [
    "# Training loop over epochs\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    print('Epoch', i)\n",
    "    start_time = timer()\n",
    "    sum_loss = 0\n",
    "\n",
    "    # Loop over batches\n",
    "    for j in np.arange(0, n_samples, batch_size):\n",
    "        batch_input = train_input[j:j+batch_size]\n",
    "        batch_labels = train_labels[j:j+batch_size]\n",
    "        model.zero_grad()\n",
    "        batch_logits = model(batch_input)\n",
    "        batch_loss = cost_function(batch_logits, batch_labels)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        sum_loss += batch_loss\n",
    "    \n",
    "    end_time = timer()\n",
    "    avg_loss = sum_loss.data.numpy()[0] / n_batches\n",
    "    print('  average loss', avg_loss, 'time %gs' % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train loss 0.618681\n"
     ]
    }
   ],
   "source": [
    "# Calculate full training set predictions\n",
    "train_logits = model(train_input)\n",
    "train_preds = logits_to_probs(train_logits)\n",
    "train_loss = cost_function(train_logits, train_labels)\n",
    "\n",
    "print('Final train loss', train_loss.data.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy\n",
    "(train_logits.max(dim=2)[1] == train_labels) / train_labels.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw an event\n",
    "i = 6\n",
    "draw_2d_input_and_pred(train_input[i].data.numpy(), train_preds[i].data.numpy(), cmap='gray_r');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
