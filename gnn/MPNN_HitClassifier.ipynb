{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message-passing graph neural network for hit classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training concurrency\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System imports\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Externals\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Local imports\n",
    "from estimator import Estimator\n",
    "from acts import process_hits_files, select_barrel_hits\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    np_to_torch = lambda x, volatile=False: (\n",
    "        Variable(torch.from_numpy(x.astype(np.float32)), volatile=volatile).cuda())\n",
    "else:\n",
    "    np_to_torch = lambda x, volatile=False: (\n",
    "        Variable(torch.from_numpy(x.astype(np.float32)), volatile=volatile))\n",
    "\n",
    "torch_to_np = lambda x: x.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "I'm deciding what I want to do as I go along.\n",
    "\n",
    "I want to demonstrate some kind of binary hit classification.\n",
    "I could use the 10-track artificial samples from the segment classification notebook,\n",
    "or I could try to build more realistic sub-graphs from real events. I think the more interesting case is the latter one.\n",
    "\n",
    "So the procedure will roughly be:\n",
    "- select all barrel hits and remove duplicates\n",
    "- select good target track samples (perfect 10-layers)\n",
    "- for each track, select a fixed number of hits in the neighborhood of the true hits\n",
    "\n",
    "I think I've implemented most of this already in other notebooks.\n",
    "- selecting closest hits was done in tree_search/RNNHitClassifier.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_hits(hits):\n",
    "    # Select all barrel hits\n",
    "    vids = [8, 13, 17]\n",
    "    hits = hits[np.logical_or.reduce([hits.volid == v for v in vids])]\n",
    "    # Re-enumerate the volume and layer numbers for convenience\n",
    "    volume = pd.Series(-1, index=hits.index, dtype=np.int8)\n",
    "    vid_groups = hits.groupby('volid')\n",
    "    for i, v in enumerate(vids):\n",
    "        volume[vid_groups.get_group(v).index] = i\n",
    "    # This assumes 4 layers per volume (except last volume)\n",
    "    layer = (hits.layid / 2 - 1 + volume * 4).astype(np.int8)\n",
    "    # Select the columns we need\n",
    "    hits = (hits[['evtid', 'barcode', 'r', 'phi', 'z']]\n",
    "            .assign(volume=volume, layer=layer))\n",
    "    # Remove duplicate hits\n",
    "    hits = hits.loc[\n",
    "        hits.groupby(['evtid', 'barcode', 'layer'], as_index=False).r.idxmin()\n",
    "    ]\n",
    "    return hits\n",
    "\n",
    "def select_signal_hits(hits):\n",
    "    # Filter tracks that hit every layer\n",
    "    hits = (hits.groupby(['evtid', 'barcode'])\n",
    "            .filter(lambda x: len(x.layer.unique()) == 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/bigdata/shared/ACTS/prod_mu10_pt1000_2017_07_29'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = os.listdir(input_dir)\n",
    "hits_files = sorted(f for f in all_files if f.startswith('clusters'))\n",
    "\n",
    "n_files = 1\n",
    "hits_files = [os.path.join(input_dir, hf) for hf in hits_files[:n_files]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /bigdata/shared/ACTS/prod_mu10_pt1000_2017_07_29/clusters_1.csv\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n_workers = 1\n",
    "with mp.Pool(processes=n_workers) as pool:\n",
    "    hits = process_hits_files(hits_files, pool)\n",
    "    print('Applying selections')\n",
    "    hits = pool.map(select_hits, hits)\n",
    "\n",
    "# Concatenate everything together\n",
    "hits = pd.concat(hits, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
